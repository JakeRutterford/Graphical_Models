{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference in Hidden Markov Models (HMMs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import beta\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have a first order Hidden Markov Model with hidden states $h_t$ and observed states $v_t$. We may be interested in the probability of the latent variables $h_t$ given the observations $v_t$, inference problems of this type can be classified as follows:\n",
    "\n",
    "* **Filtering:** Inferring the probability of the present hidden state at time $t$ given observations to that time - $p(h_t \\ | \\ v_{1:t})$\n",
    "* **Smoothing:** Inferring the probability of a past hidden state at some time $t$ given observations up to and in the future of t - $p(h_t \\ | \\ v_{1:s})$ for $t < s$\n",
    "* **Most likely path:** Inferring the most likely sequence of hidden states given a set of observations - $argmax_{\\ h1:t} \\ p(h_{1:t} \\ | \\ v_{1:t})$\n",
    "\n",
    "<img src=\"http://recognize-speech.com/images/barber2014/barber2014.fig.23.4.png\", width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a 100 timestep HMM such that each hidden state $h_t$ is binary (i.e. 0 or 1), while the observed states $v_t$ may take 3 values (0, 1 or 2). Next we need to specify the marginal distribution $P(h_1)$ and the transition matrix $P(h_t = i \\ | \\ h_{t-1} = j) = P_{ij}$. We also need an emission matrix to generate observations relating to our hidden states, we define this matrix as $A_{ij} = P(v_t = i \\ | \\ h_t = j)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = 2 # number of hidden states\n",
    "M = 3 # number of observed states\n",
    "T = 100 # number of timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create random marginal for h1\n",
    "pi = np.random.rand(N)\n",
    "pi = pi/np.sum(pi)\n",
    "\n",
    "# Create transition matrix \n",
    "P = np.array([[0.7,0.2],[0.3,0.8]])\n",
    "\n",
    "# Create emission matrix\n",
    "A = np.array([[1./3,1./2],[1./3,0.0],[1./3,1./2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marginal of h1 = \n",
      " [ 0.3287607  0.6712393]\n",
      "Transition matrix P = \n",
      " [[ 0.7  0.2]\n",
      " [ 0.3  0.8]]\n",
      "Emission Matrix A = \n",
      " [[ 0.33333333  0.5       ]\n",
      " [ 0.33333333  0.        ]\n",
      " [ 0.33333333  0.5       ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Marginal of h1 = \\n\", pi)\n",
    "print(\"Transition matrix P = \\n\",P)\n",
    "print(\"Emission Matrix A = \\n\", A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to ensure we're all on the same page, let's describe what's happening with these. The marginal matrix defines our probability of being in each possible hidden state at step 1, the emission matrix then uses the probabilities relating to this hidden state to generate observations, finally at the next timestep we transition using the matrix p based on our previous hidden state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_data(pi, P, A, T):\n",
    "    # Arrays for hidden and observed states\n",
    "    H = np.zeros(T, dtype = int)\n",
    "    V = np.zeros(T, dtype = int)\n",
    "    n = len(pi)\n",
    "    m = A.shape[0]\n",
    "    for t in range(T):\n",
    "        # Generate transition for next timestep\n",
    "        if t == 0:\n",
    "            H[t] = np.random.choice(n, p = pi)\n",
    "        else:\n",
    "            H[t] = np.random.choice(n, p = P[:,H[t-1]])\n",
    "        # Generate observed state based on current hidden state\n",
    "        V[t] = np.random.choice(m, p = A[:,H[t]])\n",
    "    return (H,V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate data for 100 timesteps\n",
    "(H,V) = generate_data(pi, P, A, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden states: \n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 0 0 0 0 0 0 0 1 1 1 0 0 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 0 0 0 1 0 0 0 1 1 1 1 1 0 0 0 1\n",
      " 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 0 0 1 1 1 1 1 1 1 1]\n",
      "Observed states: \n",
      " [0 2 0 0 2 0 0 2 2 0 0 0 2 2 0 2 1 1 0 2 0 0 0 1 1 2 1 0 2 0 0 2 1 0 2 0 2\n",
      " 2 2 0 2 2 2 0 2 0 2 2 0 0 2 2 2 2 0 0 2 2 2 0 1 0 2 1 1 0 0 2 0 2 0 1 0 2\n",
      " 0 2 0 0 0 2 0 0 2 2 1 1 2 2 2 1 0 0 2 0 2 2 2 2 0 2]\n"
     ]
    }
   ],
   "source": [
    "print(\"Hidden states: \\n\", H)\n",
    "print(\"Observed states: \\n\", V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose we are interesting in filtering, i.e. $p(h_t | v_{1:t}) \\propto p(h_t, v_{1:t})$. Computing this directly would require marginalizing over all possible previous hidden states $h_{1:t-1}$, the number of which grows exponentially with t. Instead we can make use of the forward algorithm, which takes advantage of the conditional independence rules of the HMM to perform marginalisation recursively. To demonstrate the recursion, let\n",
    "\n",
    "$$\\alpha_t(h_t) = p(h_t, v_{1:t}) = \\sum_{h_{t-1}} p(h_t, h_{t-1}, v_{1:t})$$\n",
    "\n",
    "Using the chain rule to expand $p(h_t, h_{t-1}, v_{1:t})$, we can write\n",
    "\n",
    "$$\\alpha_t(h_t) = \\sum_{h_{t-1}} p(v_t \\ | \\ h_t, h_{t-1}, v_{1:t-1} ) \\ p(h_t \\ | \\ h_{t-1}, v_{1:t-1}) \\ p(h_{t-1}, v_{1:t-1})$$\n",
    "\n",
    "Because $v_t$ is conditionally independent of everything but $h_t$, and $h_t$ is conditionally independent of everything but $h_{{t-1}}$, this simplifies to\n",
    "\n",
    "$$\\alpha_t(h_t) = p(v_t \\ | \\ h_t) \\sum_{h_{t-1}} p(h_t \\ | \\ h_{t-1}) \\ \\alpha_{t-1}(h_{t-1})$$\n",
    "\n",
    "where $p(v_t \\ | \\ h_t)$ is the emission matrix and $p(h_t \\ | \\ h_{t-1})$ is the transition matrix. Here the total number of summations is $(t-1)k$ for k hidden states, rather than $k^t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_messages(pi, P, A, V):\n",
    "    T = len(V)\n",
    "    alpha = np.zeros([N, T])\n",
    "    alpha[:, 0] = A[V[0], :] * pi\n",
    "    for t in range(1,T):\n",
    "        alpha[:, t] = A[V[t]] * np.sum(P * alpha[:,t-1], axis=1)\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(h100 | v1:100) = \n",
      " [ 0.23811172  0.76188828]\n",
      "True h100 is 1\n"
     ]
    }
   ],
   "source": [
    "# Calculate forward messages and grab last alpha\n",
    "alphas = forward_messages(pi, P, A, V)\n",
    "last_alpha = alphas[:, -1]\n",
    "\n",
    "print(\"P(h100 | v1:100) = \\n\", last_alpha/np.sum(last_alpha))\n",
    "print(\"True h100 is\", H[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose now that we are interested in inferring the hidden state at some arbitrary time in the past, i.e. $p(h_t \\ | \\ v_{1:T})$ for $t<T$. We therefore want to evaluate\n",
    "\n",
    "$$p(h_t \\ | \\ v_{1:T}) = p(h_t \\ | \\ v_{1:t}) \\ p(v_{t+1:T} \\ | \\ h_t)$$\n",
    "\n",
    "Where the left hand terms $p(h_t \\ | \\ v_{1:t})$ are the forward recursions we calculated previously. We therefore need $p(v_{t+1:T} \\ | \\ h_t)$, which using a similar approach to above we calculate the backwards recursions as:\n",
    "\n",
    "$p(v_{t+1:T} \\ | \\ h_t) = \\sum_{h_{t+1}} p(v_{t+1:T}, h_{t+1}\\ | \\ h_t) = \\sum_{h_{t+1}} p(v_{t+2:T} \\ | \\ h_{t+1}) \\ p(v_{t+1} \\ | \\ h_{t+1}) \\ p(h_{t+1} \\ | \\ h_t)$\n",
    "\n",
    "Which has the following recursion\n",
    "\n",
    "$\\beta_t(h_t) = \\sum_{h_{t+1}} \\beta_{t+1}(h_{t+1}) \\ p(v_{t+1} \\ | \\ h_{t+1}) \\ p(h_{t+1} \\ | \\ h_t)$\n",
    "\n",
    "where $\\beta_T(h_T) = 1$. \n",
    "\n",
    "The marginal $p(h_t \\ | \\ v_{1:T})$ is then calculated by taking the product of the forward and backward messages to $h_t$ and normalising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def backward_messages(pi, P, A, V):\n",
    "    T = len(V)\n",
    "    beta = np.zeros([N, T])\n",
    "    beta[:,T-1] = 1.0\n",
    "    for t in range(T-1, 0, -1):\n",
    "        # Take care of summation using matrix multiplication\n",
    "        beta[:, t-1] = beta[:,t] * A[V[t]] @ P\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(h1   | v1) = \n",
      " [ 0.24614844  0.75385156]\n",
      "P(v2:T | h1) = \n",
      " [ 0.41265116  0.58734884]\n",
      "P(h1   | v1:T) = \n",
      " [ 0.18659677  0.81340323]\n"
     ]
    }
   ],
   "source": [
    "# calculate backwards messages\n",
    "betas = backward_messages(pi,P,A,V)\n",
    "\n",
    "# calculate forward and backward messages to h1\n",
    "print(\"P(h1   | v1) = \\n\", alphas[:,0]/np.sum(alphas[:,0]))\n",
    "print(\"P(v2:T | h1) = \\n\", betas[:,0]/np.sum(betas[:,0]))\n",
    "\n",
    "# take the product of these messages and normalise\n",
    "Pt = alphas[:,0] * betas[:,0]\n",
    "print(\"P(h1   | v1:T) = \\n\", Pt/np.sum(Pt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def marginal(alphas, betas, ht):\n",
    "    'Return normalised product of forward and backward messages for some ht'\n",
    "    Pt = alphas[:, ht] * betas[:, ht]\n",
    "    return Pt/np.sum(Pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(h100 | v1:100) = \n",
      " [ 0.23811172  0.76188828]\n",
      "P(h50  | v1:100) = \n",
      " [ 0.17996299  0.82003701]\n",
      "P(h1   | v1:100) = \n",
      " [ 0.18659677  0.81340323]\n"
     ]
    }
   ],
   "source": [
    "'Some probabilities we can now calculate'\n",
    "print(\"P(h100 | v1:100) = \\n\", marginal(alphas, betas, 99))\n",
    "print(\"P(h50  | v1:100) = \\n\", marginal(alphas, betas, 49))\n",
    "print(\"P(h1   | v1:100) = \\n\", marginal(alphas, betas, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
